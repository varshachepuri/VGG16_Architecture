{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Task-1 : Implement VGG16 on Food101 dataset.\nYour first task would be to implement the VGG16 architecture model class and train a classification model on the Food101 dataset using the above architecture. The details of the dataset are given below.","metadata":{}},{"cell_type":"markdown","source":"1.1 Import packages\nSome packages are imported. However, you would need to import any other package that is required in the implementation that you feel is required. But do keep in mind, your model shouldn't be imported. It has to be implemented using the basic convolution layers.","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport random\nimport os\nfrom torch.utils.data import Dataset,DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-25T09:27:22.623840Z","iopub.execute_input":"2024-02-25T09:27:22.624107Z","iopub.status.idle":"2024-02-25T09:27:33.401378Z","shell.execute_reply.started":"2024-02-25T09:27:22.624082Z","shell.execute_reply":"2024-02-25T09:27:33.400434Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"1.2. Dataset\nThe Food-101 is a challenging data set of 101 food categories with 101,000 images. All images were rescaled to have a maximum side length of 512 pixels. Implementing the below cell will allow you to download the dataset into your colab directory under /data/food-101. Inside the directory you would find the information about the dataset and also a ReadMe.txt file.\n\nNow, the image size of the dataset is (512,512,3). However, the model expects the image to be of the size (224,224,3). Now using the transforms method, write a composed transformation where you implement the resize as well as convert to tensor function. Do some basic preprocessing as well, Normalisation, Standardization etc.\nHint : use the transform.Compose() method.","metadata":{}},{"cell_type":"code","source":"### YOUR CODE STARTS HERE ###\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resizing the image to (224, 224)\n    transforms.ToTensor(),  \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing the image\n])\n\n### YOUR CODE ENDS HERE ###","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:27:41.466931Z","iopub.execute_input":"2024-02-25T09:27:41.467403Z","iopub.status.idle":"2024-02-25T09:27:41.473141Z","shell.execute_reply.started":"2024-02-25T09:27:41.467373Z","shell.execute_reply":"2024-02-25T09:27:41.472262Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Setup training data\ntrain_data = datasets.Food101(\n    root=\"data\",\n    split=\"train\", # get training data\n    download=True,\n    transform=transform\n)\n\n# Setup testing data\ntest_data = datasets.Food101(\n    root=\"data\",\n    split=\"test\", # get test data\n    download=True,\n    transform=transform\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:27:49.971164Z","iopub.execute_input":"2024-02-25T09:27:49.971515Z","iopub.status.idle":"2024-02-25T09:32:45.168809Z","shell.execute_reply.started":"2024-02-25T09:27:49.971487Z","shell.execute_reply":"2024-02-25T09:32:45.168028Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4996278331/4996278331 [03:48<00:00, 21843702.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/food-101.tar.gz to data\n","output_type":"stream"}]},{"cell_type":"code","source":"image,label = train_data[0]\nimage ,label","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:33:58.580208Z","iopub.execute_input":"2024-02-25T09:33:58.580588Z","iopub.status.idle":"2024-02-25T09:33:58.596200Z","shell.execute_reply.started":"2024-02-25T09:33:58.580534Z","shell.execute_reply":"2024-02-25T09:33:58.595436Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 2.1119,  2.0948,  2.1290,  ..., -0.4911, -0.6109, -0.5424],\n          [ 2.1290,  2.1119,  2.1290,  ..., -0.4054, -0.4568, -0.4568],\n          [ 2.1462,  2.1290,  2.1462,  ..., -0.3883, -0.3712, -0.4397],\n          ...,\n          [ 0.5193,  0.4851,  0.4679,  ...,  0.2796,  0.2796,  0.3138],\n          [ 0.4851,  0.4337,  0.4166,  ...,  0.4166,  0.3994,  0.4508],\n          [ 0.4337,  0.3652,  0.3652,  ...,  0.4851,  0.4679,  0.4851]],\n \n         [[ 2.2185,  2.2010,  2.2360,  ..., -1.3704, -1.5105, -1.4755],\n          [ 2.2360,  2.2185,  2.2360,  ..., -1.3004, -1.3704, -1.4055],\n          [ 2.2535,  2.2360,  2.2535,  ..., -1.3004, -1.3354, -1.4230],\n          ...,\n          [-0.6877, -0.7227, -0.7402,  ...,  0.1702,  0.2227,  0.3102],\n          [-0.7052, -0.7577, -0.7752,  ...,  0.3102,  0.3627,  0.4503],\n          [-0.7577, -0.8277, -0.8277,  ...,  0.3978,  0.4328,  0.4853]],\n \n         [[ 2.4483,  2.4308,  2.4657,  ..., -1.4210, -1.6127, -1.6127],\n          [ 2.4657,  2.4483,  2.4657,  ..., -1.3164, -1.4384, -1.5081],\n          [ 2.4831,  2.4657,  2.4831,  ..., -1.3164, -1.3687, -1.4733],\n          ...,\n          [-1.4733, -1.5256, -1.5604,  ...,  0.1128,  0.1825,  0.2522],\n          [-1.4907, -1.5430, -1.5953,  ...,  0.2696,  0.3219,  0.4091],\n          [-1.5430, -1.6302, -1.6476,  ...,  0.3742,  0.3916,  0.4439]]]),\n 23)"},"metadata":{}}]},{"cell_type":"code","source":"image.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:35:35.118507Z","iopub.execute_input":"2024-02-25T09:35:35.119266Z","iopub.status.idle":"2024-02-25T09:35:35.124974Z","shell.execute_reply.started":"2024-02-25T09:35:35.119236Z","shell.execute_reply":"2024-02-25T09:35:35.124132Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"markdown","source":"1.3. Prepare Dataloader\nNow, in the cell below implement the DataLoader function for the train and test data. You then have to print the length of the train and test dataloaders.","metadata":{}},{"cell_type":"code","source":"\n\nBATCH_SIZE = 32\n\n### YOUR CODE STARTS HERE ###\n#splitting data set into batches of 32\ntrain_dataloader = DataLoader(train_data,batch_size = BATCH_SIZE,shuffle = True)\ntest_dataloader = DataLoader(test_data,batch_size = BATCH_SIZE,shuffle = True)\n### YOUR CODE ENDS HERE ###\n\n\n\nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:36:04.596727Z","iopub.execute_input":"2024-02-25T09:36:04.597090Z","iopub.status.idle":"2024-02-25T09:36:04.605072Z","shell.execute_reply.started":"2024-02-25T09:36:04.597062Z","shell.execute_reply":"2024-02-25T09:36:04.602608Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Length of train dataloader: 2368 batches of 32\nLength of test dataloader: 790 batches of 32\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:36:15.874993Z","iopub.execute_input":"2024-02-25T09:36:15.875973Z","iopub.status.idle":"2024-02-25T09:36:16.052643Z","shell.execute_reply.started":"2024-02-25T09:36:15.875940Z","shell.execute_reply":"2024-02-25T09:36:16.051494Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 3, 224, 224]), torch.Size([32]))"},"metadata":{}}]},{"cell_type":"markdown","source":"1.4. VGG16 Architecture\nNow, create a model class and implement the VGG16 architecture. The architecture layer is as follows :\n\nVGG16 takes input tensor size as 224, 244 with 3 RGB channel. It has 13 convolutional layers, 5 Max Pooling layers, and 3 Dense layers which sum up to 21 layers.\n\n\nImplement the model class in the given cell below. DONOT change the class name as that would be required in the next cell.\n\n","metadata":{}},{"cell_type":"code","source":"\n\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(VGG16, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        #self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(4096),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(4096),\n            \n            #nn.Linear(4096, num_classes),\n            #nn.Softmax(dim=1)  # Softmax layer for classification\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T10:21:05.405923Z","iopub.execute_input":"2024-02-25T10:21:05.406281Z","iopub.status.idle":"2024-02-25T10:21:05.422863Z","shell.execute_reply.started":"2024-02-25T10:21:05.406251Z","shell.execute_reply":"2024-02-25T10:21:05.422037Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"1.5. Model Training\nTrain the above defined model using the following configurations :\n\n#epochs = 20\nlearning rate = 0.05\nloss = cross entropy\noptimizer = Adam\nAfter training, save the model with the name : food101_vgg16_model.pt","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, num_epochs, learning_rate):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    total_step = len(train_loader)\n    for epoch in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if (i+1) % 100 == 0:\n                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n    # Saving the model\n    torch.save(model.state_dict(), 'food101_vgg16_model.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T10:21:11.290044Z","iopub.execute_input":"2024-02-25T10:21:11.290407Z","iopub.status.idle":"2024-02-25T10:21:11.298203Z","shell.execute_reply.started":"2024-02-25T10:21:11.290379Z","shell.execute_reply":"2024-02-25T10:21:11.296907Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# num_classes = 101\nnum_epochs = 5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlearning_rate = 0.0001\nmodel = VGG16(num_classes).to(device)\n\n\n### YOUR CODE STARTS HERE ###\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# Training the model\ntrain_model(model, train_loader, num_epochs, learning_rate)\n\n\n### YOUR CODE ENDS HERE ###\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:06:34.763367Z","iopub.execute_input":"2024-02-25T12:06:34.764125Z","iopub.status.idle":"2024-02-25T13:53:12.742326Z","shell.execute_reply.started":"2024-02-25T12:06:34.764092Z","shell.execute_reply":"2024-02-25T13:53:12.741202Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch [1/5], Step [100/2368], Loss: 7.7038\nEpoch [1/5], Step [200/2368], Loss: 7.3633\nEpoch [1/5], Step [300/2368], Loss: 7.4514\nEpoch [1/5], Step [400/2368], Loss: 6.9226\nEpoch [1/5], Step [500/2368], Loss: 7.1151\nEpoch [1/5], Step [600/2368], Loss: 7.2933\nEpoch [1/5], Step [700/2368], Loss: 7.2007\nEpoch [1/5], Step [800/2368], Loss: 7.3042\nEpoch [1/5], Step [900/2368], Loss: 6.8691\nEpoch [1/5], Step [1000/2368], Loss: 6.7475\nEpoch [1/5], Step [1100/2368], Loss: 6.0768\nEpoch [1/5], Step [1200/2368], Loss: 6.5275\nEpoch [1/5], Step [1300/2368], Loss: 6.9691\nEpoch [1/5], Step [1400/2368], Loss: 6.9699\nEpoch [1/5], Step [1500/2368], Loss: 6.0810\nEpoch [1/5], Step [1600/2368], Loss: 6.2864\nEpoch [1/5], Step [1700/2368], Loss: 6.0112\nEpoch [1/5], Step [1800/2368], Loss: 5.8773\nEpoch [1/5], Step [1900/2368], Loss: 5.9984\nEpoch [1/5], Step [2000/2368], Loss: 6.1921\nEpoch [1/5], Step [2100/2368], Loss: 5.3402\nEpoch [1/5], Step [2200/2368], Loss: 6.4702\nEpoch [1/5], Step [2300/2368], Loss: 6.5064\nEpoch [2/5], Step [100/2368], Loss: 5.4488\nEpoch [2/5], Step [200/2368], Loss: 6.0383\nEpoch [2/5], Step [300/2368], Loss: 5.7213\nEpoch [2/5], Step [400/2368], Loss: 5.4975\nEpoch [2/5], Step [500/2368], Loss: 5.5162\nEpoch [2/5], Step [600/2368], Loss: 6.0101\nEpoch [2/5], Step [700/2368], Loss: 4.6825\nEpoch [2/5], Step [800/2368], Loss: 5.7575\nEpoch [2/5], Step [900/2368], Loss: 6.4229\nEpoch [2/5], Step [1000/2368], Loss: 5.5926\nEpoch [2/5], Step [1100/2368], Loss: 5.6156\nEpoch [2/5], Step [1200/2368], Loss: 5.3707\nEpoch [2/5], Step [1300/2368], Loss: 5.7024\nEpoch [2/5], Step [1400/2368], Loss: 5.3776\nEpoch [2/5], Step [1500/2368], Loss: 5.7169\nEpoch [2/5], Step [1600/2368], Loss: 5.2164\nEpoch [2/5], Step [1700/2368], Loss: 4.8334\nEpoch [2/5], Step [1800/2368], Loss: 4.9099\nEpoch [2/5], Step [1900/2368], Loss: 5.2861\nEpoch [2/5], Step [2000/2368], Loss: 5.4036\nEpoch [2/5], Step [2100/2368], Loss: 5.1530\nEpoch [2/5], Step [2200/2368], Loss: 5.1968\nEpoch [2/5], Step [2300/2368], Loss: 5.0887\nEpoch [3/5], Step [100/2368], Loss: 4.8452\nEpoch [3/5], Step [200/2368], Loss: 4.8553\nEpoch [3/5], Step [300/2368], Loss: 5.1792\nEpoch [3/5], Step [400/2368], Loss: 5.0709\nEpoch [3/5], Step [500/2368], Loss: 4.5778\nEpoch [3/5], Step [600/2368], Loss: 4.1938\nEpoch [3/5], Step [700/2368], Loss: 4.5656\nEpoch [3/5], Step [800/2368], Loss: 5.0376\nEpoch [3/5], Step [900/2368], Loss: 4.0150\nEpoch [3/5], Step [1000/2368], Loss: 4.7475\nEpoch [3/5], Step [1100/2368], Loss: 4.4830\nEpoch [3/5], Step [1200/2368], Loss: 5.2349\nEpoch [3/5], Step [1300/2368], Loss: 4.7700\nEpoch [3/5], Step [1400/2368], Loss: 4.8522\nEpoch [3/5], Step [1500/2368], Loss: 4.8149\nEpoch [3/5], Step [1600/2368], Loss: 4.5615\nEpoch [3/5], Step [1700/2368], Loss: 4.2584\nEpoch [3/5], Step [1800/2368], Loss: 4.2718\nEpoch [3/5], Step [1900/2368], Loss: 4.2079\nEpoch [3/5], Step [2000/2368], Loss: 4.0460\nEpoch [3/5], Step [2100/2368], Loss: 4.0309\nEpoch [3/5], Step [2200/2368], Loss: 4.2984\nEpoch [3/5], Step [2300/2368], Loss: 3.7267\nEpoch [4/5], Step [100/2368], Loss: 3.9690\nEpoch [4/5], Step [200/2368], Loss: 3.6131\nEpoch [4/5], Step [300/2368], Loss: 3.8339\nEpoch [4/5], Step [400/2368], Loss: 4.4301\nEpoch [4/5], Step [500/2368], Loss: 4.3344\nEpoch [4/5], Step [600/2368], Loss: 4.1767\nEpoch [4/5], Step [700/2368], Loss: 4.3698\nEpoch [4/5], Step [800/2368], Loss: 3.7909\nEpoch [4/5], Step [900/2368], Loss: 4.5419\nEpoch [4/5], Step [1000/2368], Loss: 3.8561\nEpoch [4/5], Step [1100/2368], Loss: 4.4405\nEpoch [4/5], Step [1200/2368], Loss: 4.1054\nEpoch [4/5], Step [1300/2368], Loss: 3.7984\nEpoch [4/5], Step [1400/2368], Loss: 3.4577\nEpoch [4/5], Step [1500/2368], Loss: 3.6107\nEpoch [4/5], Step [1600/2368], Loss: 3.9271\nEpoch [4/5], Step [1700/2368], Loss: 3.1923\nEpoch [4/5], Step [1800/2368], Loss: 4.3291\nEpoch [4/5], Step [1900/2368], Loss: 4.4560\nEpoch [4/5], Step [2000/2368], Loss: 4.2165\nEpoch [4/5], Step [2100/2368], Loss: 3.9269\nEpoch [4/5], Step [2200/2368], Loss: 3.7070\nEpoch [4/5], Step [2300/2368], Loss: 4.1261\nEpoch [5/5], Step [100/2368], Loss: 3.9591\nEpoch [5/5], Step [200/2368], Loss: 3.4836\nEpoch [5/5], Step [300/2368], Loss: 3.7684\nEpoch [5/5], Step [400/2368], Loss: 3.1666\nEpoch [5/5], Step [500/2368], Loss: 3.9476\nEpoch [5/5], Step [600/2368], Loss: 4.4044\nEpoch [5/5], Step [700/2368], Loss: 3.9267\nEpoch [5/5], Step [800/2368], Loss: 3.8665\nEpoch [5/5], Step [900/2368], Loss: 3.4306\nEpoch [5/5], Step [1000/2368], Loss: 3.8694\nEpoch [5/5], Step [1100/2368], Loss: 3.3568\nEpoch [5/5], Step [1200/2368], Loss: 4.1942\nEpoch [5/5], Step [1300/2368], Loss: 2.5279\nEpoch [5/5], Step [1400/2368], Loss: 3.2833\nEpoch [5/5], Step [1500/2368], Loss: 4.2941\nEpoch [5/5], Step [1600/2368], Loss: 3.6571\nEpoch [5/5], Step [1700/2368], Loss: 4.4778\nEpoch [5/5], Step [1800/2368], Loss: 3.4647\nEpoch [5/5], Step [1900/2368], Loss: 3.4911\nEpoch [5/5], Step [2000/2368], Loss: 3.5011\nEpoch [5/5], Step [2100/2368], Loss: 2.9947\nEpoch [5/5], Step [2200/2368], Loss: 3.1249\nEpoch [5/5], Step [2300/2368], Loss: 3.1389\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1.6. Evaluate the model\nLoad the trained model and evaluate on the test data.","metadata":{}},{"cell_type":"code","source":"### YOUR CODE STARTS HERE ###\n# Define the evaluation function\ndef evaluate_model(model, test_loader):\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    with torch.no_grad():  # Disable gradient calculation\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print('Accuracy of the model on the test images: {:.2f}%'.format(accuracy))\n\n# Initialize the test DataLoader\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n# Evaluate the model\nevaluate_model(model, test_loader)\n\n### YOUR CODE ENDS HERE ###","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:54:59.056234Z","iopub.execute_input":"2024-02-25T13:54:59.056918Z","iopub.status.idle":"2024-02-25T13:58:57.899757Z","shell.execute_reply.started":"2024-02-25T13:54:59.056885Z","shell.execute_reply":"2024-02-25T13:58:57.898799Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy of the model on the test images: 42.00%\n","output_type":"stream"}]}]}